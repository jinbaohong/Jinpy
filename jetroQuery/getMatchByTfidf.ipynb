{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer  \n",
    "import os.path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def getMatchByTfidf(corpus, query):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfs = tfidf.fit_transform(corpus)\n",
    "\n",
    "    response = tfidf.transform([query])\n",
    "#     feature_names = tfidf.get_feature_names()\n",
    "#     for col in response.nonzero()[1]:\n",
    "#         print( feature_names[col], ' - ', response[0, col])\n",
    "#     print(np.amax(np.sum(tfs.toarray()[:,response.nonzero()[1]], axis=1)))\n",
    "#     print(tfs.toarray()[:,response.nonzero()[1]])\n",
    "    # Print the highest tfidf score\n",
    "    score = np.amax(np.sum(tfs.toarray()[:,response.nonzero()[1]], axis=1))\n",
    "    print(score)\n",
    "    # best : the location in corpus of the best score\n",
    "    best = np.argmax(np.sum(tfs.toarray()[:,response.nonzero()[1]], axis=1))\n",
    "#     if not best:\n",
    "#         # if best score is not zero, then return the corpus\n",
    "#         return 'Exception: No matching'\n",
    "    return ( corpus[best], score )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text1 = 'Cosmoprof North America Las Vegas(Cosmoprof)'\n",
    "    text2 = 'National Safety Council Congress & Expo Anaheim 2016(NSC Congress & Expo)'\n",
    "    text3 = 'NSC - National Safety Council Congress & Exposition'\n",
    "    text5 = 'International Consumer Electronics Show(2013 International CES)'\n",
    "\n",
    "    query = 'congress International CES - Consumer Electronics Show - the Source for Consumer Technologies'\n",
    "    query2 = 'safety    2016'\n",
    "    query3 = ''\n",
    "    query4 = 'you are a ass hole congress'\n",
    "\n",
    "    corpus = (text1, text2, text3, text5)\n",
    "\n",
    "    print(getMatchByTfidf(corpus, query2))\n",
    "    print(getMatchByTfidf(corpus, query3))\n",
    "    print(getMatchByTfidf(corpus, query))\n",
    "    print(getMatchByTfidf(corpus, query4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkFileExist(filePath):\n",
    "    if os.path.isfile(filePath):\n",
    "        return\n",
    "    print( \"File : \\'{}\\' doesn't exist.\".format(filePath) )\n",
    "    sys.exit()\n",
    "checkFileExist('data/exhibitMaifn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_data = pd.read_csv('fuzzyTable.csv')\n",
    "for key in set(my_data['FuzzyKey']):\n",
    "    tmp = my_data.loc[ my_data['FuzzyKey'] == key ]\n",
    "    corpus = tuple(tmp['EXHIBITION_NAME'])\n",
    "    query = tuple(tmp['展覽名稱'])[0]\n",
    "    tmpList = [key,\n",
    "               query,\n",
    "               getMatchByTfidf(corpus, query)[0],\n",
    "               getMatchByTfidf(corpus, query)[1]]\n",
    "    with open(\"output.csv\", \"a\") as fp:\n",
    "        wr = csv.writer(fp, dialect='excel', quotechar = '\"')\n",
    "        wr.writerow(tmpList)\n",
    "    print('query : %s\\nresult : %s' % \n",
    "        ( query, getMatchByTfidf(corpus, query)[0] ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_data = pd.read_csv('data/jetro1718.csv', encoding = \"ISO-8859-1\")\n",
    "corpus = tuple(my_data['query'])\n",
    "print('Vectorizing...')\n",
    "tfidf = TfidfVectorizer(stop_words=None)\n",
    "tfs = tfidf.fit_transform(corpus)\n",
    "\n",
    "\n",
    "simMat = (tfs * tfs.T).A\n",
    "\n",
    "\n",
    "print('Add into key...')\n",
    "for index in range( len(corpus) ):\n",
    "    print('Adding group %d' % index)\n",
    "    a = np.where( simMat[index] > 0.5 )[0]\n",
    "    tmpDF = pd.concat( [pd.DataFrame( ['group' + str(index)]*len(a) ),\n",
    "                        pd.DataFrame( [corpus[i] for i in a] ),\n",
    "                        pd.DataFrame( [simMat[index, i] for i in a] ),\n",
    "                        pd.DataFrame( [my_data['dataId'].loc[i] for i in a] ),\n",
    "                        pd.DataFrame( [my_data['idGroup'].loc[i] for i in a] )],\n",
    "                        axis = 1 )\n",
    "    with open('groupOutputSite_0118.csv', 'a') as f:\n",
    "        tmpDF.to_csv(f, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_data = pd.read_csv('data/exhibitMain.csv', encoding = \"ISO-8859-1\")\n",
    "corpus = tuple(my_data['text'])\n",
    "tfidf = TfidfVectorizer(stop_words=None)\n",
    "print('Vectorizing...')\n",
    "tfs = tfidf.fit_transform(corpus)\n",
    "\n",
    "queryTable = pd.read_csv('data/Jetro2017.csv')\n",
    "\n",
    "file = 'output2017.csv'\n",
    "with open(file=outputFile, \"a\") as fp:\n",
    "    wr = csv.writer(fp, dialect='excel', quotechar = '\"')\n",
    "    wr.writerow(['dataId', 'query', 'mainText', 'showId', 'Score'])\n",
    "for index in range(len(queryTable)):\n",
    "    print('Querying %d...' % index)\n",
    "    dataId = queryTable.loc[index][0]\n",
    "    query = queryTable.loc[index][1]\n",
    "    response = tfidf.transform([query])\n",
    "    score = np.amax(np.sum(tfs.toarray()[:,response.nonzero()[1]], axis=1))\n",
    "    best = np.argmax(np.sum(tfs.toarray()[:,response.nonzero()[1]], axis=1))\n",
    "    tmpList = [dataId,\n",
    "               query,\n",
    "               my_data.loc[best][1],\n",
    "               int(my_data.loc[best][0]),\n",
    "               score]\n",
    "    with open(file=outputFile, \"a\") as fp:\n",
    "        wr = csv.writer(fp, dialect='excel', quotechar = '\"')\n",
    "        wr.writerow(tmpList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def matchEngine(tfidf, tfs, query):\n",
    "    queryCleaned = cleanWords(query)\n",
    "    response = tfidf.transform([queryCleaned])\n",
    "    score = np.amax(np.sum(tfs.toarray()[:,response.nonzero()[1]], axis=1))\n",
    "    score_std = score / len(response.nonzero()[1])\n",
    "    best = np.argmax(np.sum(tfs.toarray()[:,response.nonzero()[1]], axis=1))\n",
    "    matchResult = my_data.loc[best][1]\n",
    "    hitw_tup1 = get_hitw(tfidf, queryCleaned, 1)\n",
    "    hitw_tup2 = get_hitw(tfidf, queryCleaned, 2)\n",
    "    hitw1 = hitw_tup1[1]\n",
    "    hitw2 = hitw_tup2[1]\n",
    "    hit1 = hitw_tup1[0]\n",
    "    hit2 = hitw_tup2[0]\n",
    "    contain_hit1 = containTerm(hit1, matchResult)\n",
    "    contain_hit2 = containTerm(hit2, matchResult)\n",
    "    \n",
    "    tmpList = [query,\n",
    "               matchResult,\n",
    "               int(my_data.loc[best][0]),\n",
    "               score,\n",
    "               score_std,\n",
    "               hit1,\n",
    "               hitw1,\n",
    "               contain_hit1,\n",
    "               hit2,\n",
    "               hitw2,\n",
    "               contain_hit2]\n",
    "    return tmpList\n",
    "\n",
    "# print('Vectorizing...')\n",
    "# tfidf = TfidfVectorizer()\n",
    "# tfs = tfidf.fit_transform(corpusProcessed)\n",
    "\n",
    "# query = 'Industrial Automation / HANNOVER MESSE - Leading Trade Fair for Process, Factory and Building Automation Systems and Solutions'\n",
    "# query = 'hannover automation spring hannover messe'\n",
    "# query = 'MDA-Motion, drives& Automation / HANNOVER MESSE - Leading Trade Fair for Power Transmission and Control'\n",
    "\n",
    "# query = 'MAGICONLINE veryRareWord- PROJECT'\n",
    "# query = cleanWords(query)\n",
    "# matchEngine(tfidf, tfs, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "queryFile = './data/Jetro0108_1_.csv'\n",
    "exhibitMainFile = 'data/exhibitMain0108.csv'\n",
    "outputFile = 'output/output0108_1_.csv'\n",
    "\n",
    "queryTable = pd.read_csv(queryFile)\n",
    "my_data = pd.read_csv(exhibitMainFile, encoding = \"ISO-8859-1\")\n",
    "corpusOriginal = tuple(my_data['text'])\n",
    "print('Cleaning corpus...')\n",
    "# corpusProcessed = map(cleanWords, corpusOriginal)\n",
    "corpusProcessed = [cleanWords(x) for x in corpusOriginal]\n",
    "\n",
    "print('Vectorizing...')\n",
    "tfidf = TfidfVectorizer()\n",
    "tfs = tfidf.fit_transform(corpusProcessed)\n",
    "\n",
    "with open(outputFile, \"a\") as fp:\n",
    "    wr = csv.writer(fp, dialect='excel', quotechar = '\"')\n",
    "    wr.writerow(['dataId',\n",
    "                 'query',\n",
    "                 'matchResult',\n",
    "                 'showId',\n",
    "                 'Score',\n",
    "                 'ScoreStd',\n",
    "                 'hit1',\n",
    "                 'hitw1',\n",
    "                 'contain_hit1',\n",
    "                 'hit2',\n",
    "                 'hitw2',\n",
    "                 'contain_hit2'])\n",
    "for index in range(len(queryTable)):\n",
    "    print('Querying %d...' % index)\n",
    "    dataId = queryTable.loc[index]['dataId']\n",
    "    query = queryTable.loc[index]['query']\n",
    "    tmpList = matchEngine(tfidf, tfs, query)\n",
    "    tmpList = [dataId] + tmpList\n",
    "    with open(outputFile, \"a\") as fp:\n",
    "        wr = csv.writer(fp, dialect='excel', quotechar = '\"')\n",
    "        wr.writerow(tmpList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def containTerm(term, mainResult):\n",
    "    mainResult = cleanWords(mainResult)\n",
    "    return term in mainResult\n",
    "\n",
    "mainResult = 'MAGICONLINE veryRareWord- PROJECT'\n",
    "containTerm('magica', mainResult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hitw(tfidf, query, order):\n",
    "    # hitw : high informative term weight\n",
    "    unigrams = query.split(' ')\n",
    "    dataDict = dict()\n",
    "    for i in range(len(unigrams)):\n",
    "        loc0 = tfidf.vocabulary_.get(unigrams[i])\n",
    "        if loc0 == None:\n",
    "            continue\n",
    "        dataDict[unigrams[i]] = tfidf.idf_[loc0]\n",
    "    s = [(k, dataDict[k]) for k in sorted(dataDict, key=dataDict.get, reverse=True)]\n",
    "    # tw_tup : term-weight tuple\n",
    "    if len(s) == 0:\n",
    "        return ('NoWord', 0.0)\n",
    "    if order > len(s):\n",
    "        order = len(s)\n",
    "    tw_tup = s[order - 1]\n",
    "    return tw_tup\n",
    "\n",
    "query = 'Industrial Automation / HANNOVER MESSE - Leading Trade Fair for Process, Factory and Building Automation Systems and Solutions'\n",
    "query = 'hannover automation spring hannover messe'\n",
    "query = 'MDA-Motion, drives& Automation / HANNOVER MESSE - Leading Trade Fair for Power Transmission and Control'\n",
    "\n",
    "query = 'MAGICONLINE veryRareWord- PROJECT'\n",
    "query = cleanWords(query)\n",
    "\n",
    "get_hitw(tfidf, query, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'MDA-Motion, CES fairs Drive drives& Automation / HANNOVER MESSE - Leading Trade Fair for Power Transmission and Control'\n",
    "\n",
    "text1 = 'Cosmoprof North America Las Vegas(Cosmoprof)'\n",
    "text2 = 'National Safety Council Congress & Expo Anaheim 2016(NSC Congress & Expo)'\n",
    "text3 = 'NSC - National Safety Council Congress & Exposition'\n",
    "text5 = 'International Consumer Electronics Show(2013 International CES)'\n",
    "\n",
    "text4 = 'congress International CES - Consumer Electronics Show - the Source for Consumer Technologies'\n",
    "\n",
    "def cleanWords(query):\n",
    "    try:\n",
    "        # Get tokens\n",
    "        counterTemp = CountVectorizer(ngram_range=(1,1))\n",
    "        langModTemp = counterTemp.fit_transform([query])\n",
    "        unigrams = counterTemp.get_feature_names()\n",
    "        # Remove stopwords\n",
    "        filtered_words = [word for word in unigrams if word not in stopwords.words('english')]\n",
    "        # Lemmatize or stemming\n",
    "        # This could be altered by st.stem(word)\n",
    "        # or\n",
    "        # snowball_stemmer = SnowballStemmer('english')  \n",
    "        # snowball_stemmer.stem(word)\n",
    "        wnl = WordNetLemmatizer()\n",
    "        cleanWords = [wnl.lemmatize(word, \"v\") for word in filtered_words]\n",
    "        # Concatenate\n",
    "        sentence = ' '.join(cleanWords)\n",
    "    except ValueError:\n",
    "        print('perhaps the documents only contain stop words')\n",
    "        return ' '\n",
    "    return sentence\n",
    "\n",
    "cleanWords(text4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
